# Optimizer configuration

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01

# Learning rate scheduler
use_lr_scheduler: true
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: "min"
  factor: 0.5
  patience: 10
  min_lr: 1e-6
